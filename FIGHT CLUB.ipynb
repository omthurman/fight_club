{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "here\n",
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -1.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 105         |\n",
      "|    ep_rew_mean          | -1.41       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030317191 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.5       |\n",
      "|    explained_variance   | -2.92       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.41       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0997     |\n",
      "|    value_loss           | 0.189       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 103         |\n",
      "|    ep_rew_mean          | -1.44       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030879444 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.5       |\n",
      "|    explained_variance   | -0.337      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.398      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0896     |\n",
      "|    value_loss           | 0.095       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 103        |\n",
      "|    ep_rew_mean          | -1.17      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 76         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03553684 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -30.4      |\n",
      "|    explained_variance   | -0.205     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.406     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0901    |\n",
      "|    value_loss           | 0.15       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 154\u001b[0m\n\u001b[0;32m    149\u001b[0m     model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mDEATH BOT2.zip\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     run_experiment(args) \n\u001b[0;32m    155\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mhere\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 147\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    129\u001b[0m     model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m    130\u001b[0m                 vecEnv, \n\u001b[0;32m    131\u001b[0m                 policy_kwargs\u001b[39m=\u001b[39mpolicy_kwargs, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m                 verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    135\u001b[0m                 device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    140\u001b[0m \u001b[39m# elif args.agent == \"trpo\":\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m#     model = TRPO(MlpPolicy, vecEnv, policy_kwargs=policy_kwargs, \u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39m#                  entcoeff=args.ent_coef, timesteps_per_batch=steps_per_env,\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m#                  verbose=1)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[0;32m    145\u001b[0m \u001b[39m# model=PPO.load(\"DEATH BOT.zip\", vecEnv)\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtimesteps)\n\u001b[0;32m    149\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mDEATH BOT2.zip\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:317\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    304\u001b[0m     \u001b[39mself\u001b[39m: PPOSelf,\n\u001b[0;32m    305\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PPOSelf:\n\u001b[1;32m--> 317\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    318\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    319\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    320\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    321\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    322\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    323\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    324\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    325\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    326\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    327\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    328\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:262\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    260\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 262\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:181\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mBox):\n\u001b[0;32m    179\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m--> 181\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[0;32m    183\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    185\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     45\u001b[0m         )\n\u001b[0;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m, in \u001b[0;36mTorilleWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     49\u001b[0m     \u001b[39m# Fix info being None -> info = {}\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     obs, reward, done,  _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action) \u001b[39m#need truncated\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m obs, reward, done, {}\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     10\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\torille\\envs\\gym_env.py:115\u001b[0m, in \u001b[0;36mToriEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mmake_actions(action)\n\u001b[0;32m    114\u001b[0m \u001b[39m# Get new state\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m state, terminal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mget_state()\n\u001b[0;32m    116\u001b[0m \u001b[39m# Compute reward (something we will define in other classes)\u001b[39;00m\n\u001b[0;32m    117\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reward_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mold_state, state)\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\torille\\torille.py:474\u001b[0m, in \u001b[0;36mToribashControl.get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    470\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`get_state()` or `reset()` must be followed by `make_actions`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    471\u001b[0m     )\n\u001b[0;32m    472\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_get_state \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m s, terminal, winner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_state()\n\u001b[0;32m    475\u001b[0m s \u001b[39m=\u001b[39m ToribashState(s, winner)\n\u001b[0;32m    476\u001b[0m \u001b[39mreturn\u001b[39;00m s, terminal\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\torille\\torille.py:419\u001b[0m, in \u001b[0;36mToribashControl._recv_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv_state\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    413\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39m    Read state from Toribash\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[39m        State: List of floats representing the state of the game\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m        Terminal: Boolean indicating if this is the final state of game\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_line(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection)\u001b[39m.\u001b[39mdecode()\n\u001b[0;32m    420\u001b[0m     terminal \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    421\u001b[0m     winner \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\torille\\torille.py:406\u001b[0m, in \u001b[0;36mToribashControl._recv_line\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[39mCall recv till data ends with ToribashConstant.MESSAGE_END\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[39m# First wait till there is something to read\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m ret \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39;49mrecv(constants\u001b[39m.\u001b[39;49mBUFFER_SIZE)\n\u001b[0;32m    407\u001b[0m \u001b[39m# Now check if we had \"\\n\", and continue reading till we have it\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39mwhile\u001b[39;00m ret[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:] \u001b[39m!=\u001b[39m constants\u001b[39m.\u001b[39mMESSAGE_END:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from stable_baselines import PPO2, TRPO\n",
    "# from stable_baselines3.common.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_util import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env #INTERESTING THIS ALLOWS VECTORIZED SO I DONT HAVE TO USE THAT CRAZY LAMBDA FUNCTION\n",
    "\n",
    "from torille import envs\n",
    "import gym\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "parser = ArgumentParser(\"Run stable-baselines on torille\")\n",
    "parser.add_argument(\"env\")\n",
    "parser.add_argument(\"agent\", choices=[\"ppo\"])\n",
    "parser.add_argument(\"experiment_name\")\n",
    "parser.add_argument(\"--timesteps\", type=int, default=int(3 * 1e6))\n",
    "parser.add_argument(\"--randomize_engagement\", action=\"store_true\")\n",
    "parser.add_argument(\"--turnframes\", type=int, default=5)\n",
    "parser.add_argument(\"--ent_coef\", type=float, default=0.01)\n",
    "parser.add_argument(\"--steps_per_batch\", type=int, default=1024)\n",
    "parser.add_argument(\"--num_envs\", type=int, default=1)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # Print the device, should show 'cuda' if GPU is available.\n",
    "\n",
    "torch.backends.cudnn.benchmark = True # Enable cuDNN auto-tuner to find the best algorithm to use for your hardware.\n",
    "\n",
    "\n",
    "class TorilleWrapper(gym.Wrapper): #looks like this unfucks toribash\n",
    "    \"\"\" Ad-hoc wrapper for many things with torille \"\"\"\n",
    "    def __init__(self, env, record_every_episode, record_name, randomize_settings, **kwargs):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.record_every_episode = 5\n",
    "        self.record_name = record_name\n",
    "        self.randomize_settings = randomize_settings\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Fix info being None -> info = {}\n",
    "        obs, reward, done,  _ = self.env.step(action) #need truncated\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        # Ad-hoc settings for destroyuke\n",
    "        self.env.settings.set(\"custom_settings\", 1)\n",
    "        for key,values in self.randomize_settings.items():\n",
    "            self.env.settings.set(key, random.randint(*values))\n",
    "\n",
    "        if self.num_episodes % 100 == 0: #every 100 episodes...\n",
    "            self.env.settings.set(\"replay_file\", \"%s_%d\" % (self.record_name, self.num_episodes))\n",
    "        \n",
    "        return obs\n",
    "\n",
    "class args:\n",
    "    env = 'Toribash-DestroyUke-v1'\n",
    "    agent = 'ppo'\n",
    "    experiment_name = 'Operation Bruce Two'\n",
    "    timesteps = 180000 #int(3 * 1e6)  #roughly 5.55 of these is a second . 60000 is 20 minutes\n",
    "    randomize_engagement = True\n",
    "    turnframes = 10\n",
    "    ent_coef = 0.01\n",
    "    steps_per_batch = 1024\n",
    "    num_envs = 1\n",
    "\n",
    "def run_experiment(args):\n",
    "    \n",
    "    randomization_settings = {\n",
    "        \"engagement_distance\": (100,100),\n",
    "        \"turnframes\": (args.turnframes, args.turnframes)\n",
    "    }\n",
    "\n",
    "    if args.randomize_engagement: \n",
    "        randomization_settings[\"engagement_distance\"] = (100, 200) #interesting\n",
    "    \n",
    "    vecEnv = None\n",
    "    if args.num_envs == 1:\n",
    "        # Create dummyvecenv\n",
    "        env = gym.make(args.env)\n",
    "        env.set_draw_game(False)\n",
    "        env = Monitor(TorilleWrapper(env, 100, args.experiment_name, randomization_settings), args.experiment_name)\n",
    "        env.set_draw_game(False)\n",
    "\n",
    "        vecEnv = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run           #fuck does that mean? does this unfuck the crazy toribash variables?\n",
    "        # vecEnv.set_draw_game(False)\n",
    "    else:\n",
    "        vecEnv = []\n",
    "        \n",
    "        def make_env():\n",
    "            env = gym.make(args.env)\n",
    "            unique_id = str(time.time())[-6:]\n",
    "            experiment_env_name = args.experiment_name + (\"_env%s\" % unique_id)\n",
    "            return Monitor(TorilleWrapper(env, 100, experiment_env_name, randomization_settings), \n",
    "                           experiment_env_name)\n",
    "        \n",
    "        for i in range(args.num_envs):\n",
    "            vecEnv.append(make_env)\n",
    "        \n",
    "        vecEnv = SubprocVecEnv(vecEnv)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    steps_per_env = args.steps_per_batch // args.num_envs\n",
    "\n",
    "\n",
    "\n",
    "    # Standard 2 x 64 network with sigmoid activations\n",
    "    policy_kwargs = dict( net_arch=[64, 64, 64]) #disgusting. a fucking sigmoid structure. what the actual fuck. change that to elu. sigmoid jesus christ.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model = None\n",
    "    print(\"here\")\n",
    "    if args.agent == \"ppo\":\n",
    "        model = PPO(\"MlpPolicy\", \n",
    "                    vecEnv, \n",
    "                    policy_kwargs=policy_kwargs, \n",
    "                    ent_coef=args.ent_coef, \n",
    "                    n_steps=steps_per_env,\n",
    "                    verbose=1,\n",
    "                    device=device)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # elif args.agent == \"trpo\":\n",
    "    #     model = TRPO(MlpPolicy, vecEnv, policy_kwargs=policy_kwargs, \n",
    "    #                  entcoeff=args.ent_coef, timesteps_per_batch=steps_per_env,\n",
    "    #                  verbose=1)\n",
    "\n",
    "    # model=PPO.load(\"DEATH BOT.zip\", vecEnv)\n",
    "\n",
    "    model.learn(total_timesteps=args.timesteps)\n",
    "\n",
    "    model.save(\"DEATH BOT2.zip\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    run_experiment(args) \n",
    "    print(\"here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\omthu\\AppData\\Local\\Temp\\ipykernel_15880\\247865428.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Is TensorFlow using GPU? : False\n",
      "Is PyTorch using GPU? : False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Is TensorFlow using GPU? :\", tf.test.is_gpu_available())\n",
    "\n",
    "import torch\n",
    "print(\"Is PyTorch using GPU? :\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor([[ 2.0243, -1.0856,  0.5185],\n",
      "        [ 1.9840,  0.2352, -1.6643],\n",
      "        [-1.9197,  1.4905, -1.3664]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "x = torch.randn(3, 3).to(device)\n",
    "y = torch.randn(3, 3).to(device)\n",
    "z = x.mm(y)\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5770208980103840809\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Missing required positional argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tf\u001b[39m.\u001b[39;49mdebugging\u001b[39m.\u001b[39;49massert_all_finite()\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\omthu\\my-app\\my_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1170\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[39mif\u001b[39;00m iterable_params \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m   args, kwargs \u001b[39m=\u001b[39m replace_iterable_params(args, kwargs, iterable_params)\n\u001b[1;32m-> 1170\u001b[0m result \u001b[39m=\u001b[39m api_dispatcher\u001b[39m.\u001b[39;49mDispatch(args, kwargs)\n\u001b[0;32m   1171\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1172\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mTypeError\u001b[0m: Missing required positional argument"
     ]
    }
   ],
   "source": [
    "tf.debugging.assert_all_finite()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('my_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aa45bcc386f2c7377b32fbd7616681315c7e140e806c5e190f1746b5891ca2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
